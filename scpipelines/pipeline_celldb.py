'''
===============
Pipeline celldb
===============


Overview
========

This pipeline uploads the outputs from the upstream single-cell preprocessing
steps into a SQLite database.

Usage
=====

See :ref:`` and :ref:`` for general information on how to use cgat
pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.yml` file.

Default configuration files can be generated by executing:
   cellhub celldb config

Input files
-----------

The pipeline requires the output of the pipelines:
    >> pipeline_cellranger.py : sample/10X-chip-channel x design-metadata
    >> pipeline_qc_metrics.py : barcode/cell x sequencing + mapping metadata
    >> pipeline_ambient_rna.py : gene/feature x sequencing + mapping metadata

pipeline generates a tsv configured file.

Dependencies
------------

Pipeline output
===============

The pipeline returns an SQLite populated database of metadata and
quality features that aid the selection of 'good' cells from 'bad' cells.

Currently the following tables are generated:
* metadata


Code
====

'''

from ruffus import *

import sys
import os
import re
import sqlite3
import pandas as pd

import cgatcore.experiment as E
from cgatcore import pipeline as P
import cgatcore.iotools as iotools
import cgatcore.database as database


# Load options from the config file

PARAMS = P.get_parameters(
    ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
     "../pipeline.yml",
     "pipeline.yml"])


def connect():
    '''connect to database.
    Use this method to connect to additional databases.
    Returns a database connection.
    '''

    dbh = database.connect(url=PARAMS["database_url"])

    return dbh


@transform(PARAMS['data_sample_info'],
           suffix(".tsv"),
           r"\1_csvdb.tsv")

def preprocess_metadata_sequencing(infile, outfile):
    '''Process metadata from the pipeline_cellranger.py output
    [[ it contains the column sample_id ]]'''

    df = pd.read_table(infile)
    df.columns = df.columns.str.replace(' ','_')

    global sm
    sm = ", sm.".join(df.columns)
    sm = "sm."+sm

    df.to_csv(outfile, sep='\t', index=False)

@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform(preprocess_metadata_sequencing,
           suffix(".tsv"),
           r"\1.load")

def load_metadata_sequencing(infile, outfile):
    '''load metadata of sequencing data into database '''

    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db
                          --retry
                          --database-url=sqlite:///./csvdb
                          -i "sample_id"
                          --table=metadata_sequencing
                 > %(outfile)s
              '''

    P.run(statement)

@transform(PARAMS['data_sample_info'],
           suffix(".tsv"),
           r"\1_cellranger_qc.tsv")

def preprocess_metadata_mapping(infile, outfile):
    '''Process metadata from the pipeline_cellranger.py output
    [[ it contains the column sample_id ]]'''

    samples = pd.read_csv(infile, sep='\t')
    samples.set_index("sample_id", inplace=True)

    ss = []

    for sample_name in samples.index:
        cellranger_summary = "-".join([sample_name, "count/outs/metrics_summary.csv"])
        map_sum = pd.read_csv(cellranger_summary, sep=',')
        map_sum.columns = map_sum.columns.str.replace(' |\\(|\\)|\\:','_')
        map_sum['sample_id'] = sample_name
        ss.append(map_sum)
    
    summary = pd.concat(ss)
    
    global mm
    mm = ", mm.".join(summary.columns)
    mm = "mm."+mm

    summary.to_csv(outfile, sep = '\t', index=False)
                
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform(preprocess_metadata_mapping,
           suffix(".tsv"),
           r"\1.load")

def load_metadata_mapping(infile, outfile):
    '''load metadata of mapping data into database '''

    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db
                          --retry
                          --database-url=sqlite:///./csvdb
                          -i "sample_id"
                          --table=metadata_mapping
                 > %(outfile)s
              '''

    P.run(statement)


@follows(load_metadata_sequencing)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@merge(os.path.join(PARAMS['data_scrublet'],"*_scrublet.tsv.gz"),
           "scrublet_merge.tsv")
def process_merged_scrublet(infiles, outfile):
    '''Process concatenate data together from scrublet'''

    li = []

    for fname in infiles:
        df = pd.read_table(fname, index_col=None)
        #df['barcode_id'] = df["BARCODE"].str.replace("-1", "")
        df['barcode_id'] = df['BARCODE'] + "-" + df['sample']
        li.append(df)

    frame = pd.concat(li, axis=0, ignore_index=True)
    
    global scrub
    scrub = ", scrub.".join(frame.columns)
    scrub="scrub."+scrub

    frame.to_csv(outfile, sep='\t', index=False)


@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform(process_merged_scrublet,
           suffix(".tsv"),
           ".load")
def load_merged_scrublet(infile, outfile):
    '''load scrublet output data into database '''

    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db
                          --retry
                          --database-url=sqlite:///./csvdb
                           -i "barcode_id"
                          --table=gex_scrublet
                 > %(outfile)s
              '''

    P.run(statement)


@merge(os.path.join(PARAMS['data_qcmetrics'],"*_qcmetrics.tsv.gz"),
           "qcmetrics_merge.tsv")
def process_merged_qcmetrics(infiles, outfile):
    '''Process concatenate data together from qcmetrics'''

    li = []

    for fname in infiles:
        df = pd.read_table(fname, index_col=None)
        #df['barcode_id'] = df["BARCODE"].str.replace("-1", "")
        df['barcode_id'] = df['BARCODE'] + "-" + df['sample']
        df.rename(columns={'sample':'sample_id'}, inplace = True)
        li.append(df)

    frame = pd.concat(li, axis=0, ignore_index=True)
    
    global qc
    qc = ", qc.".join(frame.columns)
    qc="qc."+qc

    frame.to_csv(outfile, sep='\t', index=False)


@follows(load_merged_scrublet)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform(process_merged_qcmetrics,
           suffix(".tsv"),
           ".load")

def load_merged_qcmetrics(infile, outfile):
    '''load qcmetrics output data into database '''

    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db
                          --retry
                          --database-url=sqlite:///./csvdb
                          -i "barcode_id"
                          -i "sample_id"
                          --table=gex_qcmetrics
                 > %(outfile)s
              '''

    P.run(statement)


#@merge(os.path.join(PARAMS['data_adt'],"*/*_qcmetrics.tsv.gz"),
#           "adtmetrics_merge.tsv")
#def process_merged_adt(infiles, outfile):
#    '''Process concatenate data together from qcmetrics'''
#
#    li = []
#
#    for fname in infiles:
#        df = pd.read_table(fname, index_col=None)
#        #df['barcode_id'] = df["BARCODE"].str.replace("-1", "")
#        df['barcode_id'] = df['BARCODE'] + "-" + df['sample']
#        li.append(df)
#
#    frame = pd.concat(li, axis=0, ignore_index=True)
#
#    frame.to_csv(outfile, sep='\t', index=False)


#@follows(load_merged_demux)
#@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
#@transform(process_merged_adt,
#           suffix(".tsv"),
#           ".load")
#def load_merged_adt(infile, outfile):
#    '''load adt metrics output data into database '''
#
#    statement='''cat %(infile)s
#                 | python -m cgatcore.csv2db
#                          --retry
#                          --database-url=sqlite:///./csvdb
#                          -i "barcode_id"
#                          -i "sample"
#                          --table=adt_metrics
#                 > %(outfile)s
#              '''

#    P.run(statement)


@follows(load_merged_qcmetrics, load_merged_scrublet, load_metadata_sequencing, load_metadata_mapping)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("final.load")
def final(outfile):
    ''' '''

    dbh = connect()

    print(sm)

    statement = "CREATE VIEW final AS \
                    SELECT " + \
                    sm + "," + \
                    qc + "," + \
                    scrub + "," + \
                    mm + \
                    " FROM metadata_sequencing sm \
                    LEFT JOIN metadata_mapping mm \
                    ON sm.sample_id = mm.sample_id \
                    LEFT JOIN gex_qcmetrics qc \
                    ON mm.sample_id = qc.sample_id \
                    LEFT JOIN gex_scrublet scrub \
                    ON qc.barcode_id = scrub.barcode_id"

    cc = database.executewait(dbh, statement, retries=5)

    cc.close()

    iotools.touch_file(outfile)


# ########################################################################### #
# ##################### full target: to run all tasks ####################### #
# ########################################################################### #

@follows(final)
def full():
    pass

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
