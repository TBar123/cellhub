'''
===============
Pipeline celldb
===============


Overview
========

This pipeline uploads the outputs from the upstream single-cell preprocessing
steps into a SQLite database.

Usage
=====

See :ref:`` and :ref:`` for general information on how to use cgat
pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.yml` file.

Default configuration files can be generated by executing:
   cellhub celldb config

Input files
-----------

The pipeline requires the output of ??. Each task of the upstream
pipeline generates a tsv configured file.

Dependencies
------------

Pipeline output
===============

The pipeline returns an SQLite populated database of metadata and
quality features that aid the selection of 'good' cells from 'bad' cells.

Currently the following tables are generated:
* metadata


Code
====

'''

from ruffus import *

import sys
import os
import re
import pandas as pd

import cgatcore.experiment as E
from cgatcore import pipeline as P
import cgatcore.iotools as iotools
import cgatcore.database as database


# Load options from the config file

PARAMS = P.get_parameters(
    ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
     "../pipeline.yml",
     "pipeline.yml"])


def connect():
    '''connect to database.
    Use this method to connect to additional databases.
    Returns a database connection.
    '''

    dbh = sqlite3.connect(PARAMS["database_url"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh



@transform(PARAMS['data_sequencing_info'],
           regex("\S+/(\S+).tsv"),
           r"\1.tsv")
def preprocess_metadata_sequencing(infile, outfile):
    '''Process metadata from sequencig core because it contains the column Index'''

    df = pd.read_table(infile)
    df.columns = df.columns.str.replace(' ','_')
    df.rename(columns={'Index':'SampleIndex'}, inplace=True)
    df.to_csv(outfile, sep='\t', index=False)
    


@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform(preprocess_metadata_sequencing,
           suffix(".tsv"),
           r"\1.load")
def load_metadata_sequencing(infile, outfile):
    '''load metadata of sequencing data into database '''


    P.load(infile, outfile,
           tablename="metadata_sequencing",
           options="--primary-key=Sequencing_ID")


@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform("metadata/*_metadata.tsv.gz",
           suffix(".tsv.gz"),
           ".load")
def load_metadata_samples(infile, outfile):
    '''load metadata of patients into database '''

    P.load(infile, outfile,
           tablename="metadata_patient",
           options="--primary-key=")



@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform("scrublet/*_scrublet.tsv.gz",
           suffix(".tsv.gz"),
           ".load")
def load_scrublet(infile, outfile):
    '''load scrublet output data into database '''

    P.load(infile, outfile,
           tablename="scrublet_%s" % P.to_table(outfile),
           options="--primary-key=barcode")


@follows(load_scrublet)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@merge("scrublet/*_scrublet.tsv.gz",
           "scrublet_merge.load")
def load_merged_scrublet(infiles, outfile):
    '''load scrublet output data into database '''


    P.concatenate_and_load(infiles, outfile,
                           tablename="merged_scrublet",
                           options="--primary-key=barcode")


@follows(load_merged_scrublet)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform("qc_metrics/*_metrics.tsv.gz",
           suffix(".tsv.gz"),
           ".load")
def load_qcmetrics(infile, outfile):
    '''load qcmetrics output data into database '''

    P.load(infile, outfile,
           tablename="qcmetrics_%s" % P.to_table(outfile),
           options="--primary-key=barcode")


@follows(load_merged_scrublet)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@merge("qc_metrics/*_metrics.tsv.gz",
           "qcmetrics_merge.load")
def load_merged_qcmetrics(infiles, outfile):
    '''load qcmetrics output data into database '''


    P.concatenate_and_load(infiles, outfile,
                           tablename="merged_qcmetric",
                           options="--primary-key=barcode")



def full():
    pass

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
