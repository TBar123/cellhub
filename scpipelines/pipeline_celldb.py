'''
===============
Pipeline celldb
===============


Overview
========

This pipeline uploads the outputs from the upstream single-cell preprocessing
steps into a SQLite database.

Usage
=====

See :ref:`` and :ref:`` for general information on how to use cgat
pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.yml` file.

Default configuration files can be generated by executing:
   cellhub celldb config

Input files
-----------

The pipeline requires the output of ??. Each task of the upstream
pipeline generates a tsv configured file.

Dependencies
------------

Pipeline output
===============

The pipeline returns an SQLite populated database of metadata and
quality features that aid the selection of 'good' cells from 'bad' cells.

Currently the following tables are generated:
* metadata


Code
====

'''

from ruffus import *

import sys
import os
import re
import sqlite3
import pandas as pd

import cgatcore.experiment as E
from cgatcore import pipeline as P
import cgatcore.iotools as iotools
import cgatcore.database as database


# Load options from the config file

PARAMS = P.get_parameters(
    ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
     "../pipeline.yml",
     "pipeline.yml"])


def connect():
    '''connect to database.
    Use this method to connect to additional databases.
    Returns a database connection.
    '''

    dbh = database.connect(url=PARAMS["database_url"])

    return dbh


@transform(PARAMS['data_sequencing_info'],
           regex("\S+/(\S+).tsv"),
           r"\1.tsv")
def preprocess_metadata_sequencing(infile, outfile):
    '''Process metadata from sequencig core because it contains the column Index'''

    df = pd.read_table(infile)
    df.columns = df.columns.str.replace(' ','_')
    df.rename(columns={'Index':'SampleIndex'}, inplace=True)
    df.to_csv(outfile, sep='\t', index=False)


@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform(preprocess_metadata_sequencing,
           suffix(".tsv"),
           r"\1.load")
def load_metadata_sequencing(infile, outfile):
    '''load metadata of sequencing data into database '''

    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db
                          --retry
                          --database-url=sqlite:///./csvdb
                          -i "Sequencing_ID"
                          --table=metadata_sequencing
                 > %(outfile)s
              '''

    P.run(statement)


@files(PARAMS['data_sequencing_info'],
       "pool_to_channels.tsv")
def extract_channel_info(infile, outfile):
    '''extract the channel info from the seq metadata'''

    x = pd.read_csv(infile, sep="\t")

    y = pd.DataFrame(x["Sequencing ID"].values, columns=["Sequencing_ID"])
    y["gPlex"] = [x.split("_")[1] for x in x["Sample Name"]]
    y["Pool"] = [x[:-1] for x in y["gPlex"]]
    y["Channel"] = [x[-1:] for x in y["gPlex"]]

    y.to_csv(outfile, sep="\t", index=False)


@follows(load_metadata_sequencing)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform(extract_channel_info,
           suffix(".tsv"),
           ".load")
def load_channels(infile, outfile):
    '''load lookup table of channel ids into database '''

    statement='''cat %(infile)s
                 | sed 's/Sequencing.ID/Sequencing_ID/g'
                 | python -m cgatcore.csv2db
                          --retry
                          --database-url=sqlite:///./csvdb
                          -i "Pool"
                          --table=channels
                 > %(outfile)s
              '''

    P.run(statement)


@follows(load_channels)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@files(PARAMS['data_lookup_combatid'],
       r"combatids.load")
def load_combatids(infile, outfile):
    '''load lookup table of combat ids into database '''

    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db
                          --retry
                          --database-url=sqlite:///./csvdb
                          -i "Pool"
                          -i "Sequencing_ID"
                          --table=combatids
                 > %(outfile)s
              '''

    P.run(statement)


@follows(load_combatids)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@files(PARAMS['data_patient_metadata'],
       "clinical_metadata.load")
def load_metadata_samples(infile, outfile):
    '''load metadata of patients into database '''


    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db
                          --retry
                          --database-url=sqlite:///./csvdb
                           -i "COMBATID"
                          --table=clinical_metadata
                 > %(outfile)s
              '''

    P.run(statement)


@merge(os.path.join(PARAMS['data_scrublet'],"*_scrublet.tsv.gz"),
           "scrublet_merge.tsv")
def process_merged_scrublet(infiles, outfile):
    '''Process concatenate data together from scrublet'''

    li = []

    for fname in infiles:
        df = pd.read_table(fname, index_col=None)
        #df['barcode_id'] = df["BARCODE"].str.replace("-1", "")
        df['barcode_id'] = df['BARCODE'] + "-" + df['sample']
        li.append(df)

    frame = pd.concat(li, axis=0, ignore_index=True)

    frame.to_csv(outfile, sep='\t', index=False)


@follows(load_metadata_samples)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform(process_merged_scrublet,
           suffix(".tsv"),
           ".load")
def load_merged_scrublet(infile, outfile):
    '''load scrublet output data into database '''

    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db
                          --retry
                          --database-url=sqlite:///./csvdb
                           -i "barcode_id"
                          --table=gex_scrublet
                 > %(outfile)s
              '''

    P.run(statement)


@merge(os.path.join(PARAMS['data_qcmetrics'],"*_qcmetrics.tsv.gz"),
           "qcmetrics_merge.tsv")
def process_merged_qcmetrics(infiles, outfile):
    '''Process concatenate data together from qcmetrics'''

    li = []

    for fname in infiles:
        df = pd.read_table(fname, index_col=None)
        #df['barcode_id'] = df["BARCODE"].str.replace("-1", "")
        df['barcode_id'] = df['BARCODE'] + "-" + df['sample']
        li.append(df)

    frame = pd.concat(li, axis=0, ignore_index=True)

    frame.to_csv(outfile, sep='\t', index=False)


@follows(load_merged_scrublet)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform(process_merged_qcmetrics,
           suffix(".tsv"),
           ".load")
def load_merged_qcmetrics(infile, outfile):
    '''load qcmetrics output data into database '''

    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db
                          --retry
                          --database-url=sqlite:///./csvdb
                          -i "barcode_id"
                          -i "sample"
                          --table=gex_qcmetrics
                 > %(outfile)s
              '''

    P.run(statement)


@merge(os.path.join(PARAMS['data_demux'],"*/*_SingleCellMetadata_demultiplexing_results.tsv.gz"),
       "demux_merged.tsv")
def process_merged_demux(infiles, outfile):
    '''Process concatenate data together from qcmetrics'''

    li = []

    for fname in infiles:
        name = os.path.basename(fname).replace("_SingleCellMetadata_demultiplexing_results.tsv.gz","")
        df = pd.read_table(fname, index_col=None)
        df.columns = ['barcode','demuxletV2_channel', 'vireo_channel', 'vireo_unknown']
        #df['barcode_id'] = df["barcode"].str.replace("-1", "")
        df['barcode_id'] = df['barcode'] + "-" + name
        df['sample'] = name
        li.append(df)

    frame = pd.concat(li, axis=0, ignore_index=True)

    frame.to_csv(outfile, sep='\t', index=False)


@follows(load_merged_qcmetrics)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform(process_merged_demux,
           suffix(".tsv"),
           ".load")
def load_merged_demux(infile, outfile):
    ''' '''

    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db
                          --retry
                          --database-url=sqlite:///./csvdb
                           -i "barcode_id"
                          --table=gex_demux
                 > %(outfile)s
              '''

    P.run(statement)


@merge(os.path.join(PARAMS['data_adt'],"*/*_qcmetrics.tsv.gz"),
           "adtmetrics_merge.tsv")
def process_merged_adt(infiles, outfile):
    '''Process concatenate data together from qcmetrics'''

    li = []

    for fname in infiles:
        df = pd.read_table(fname, index_col=None)
        #df['barcode_id'] = df["BARCODE"].str.replace("-1", "")
        df['barcode_id'] = df['BARCODE'] + "-" + df['sample']
        li.append(df)

    frame = pd.concat(li, axis=0, ignore_index=True)

    frame.to_csv(outfile, sep='\t', index=False)


@follows(load_merged_demux)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform(process_merged_adt,
           suffix(".tsv"),
           ".load")
def load_merged_adt(infile, outfile):
    '''load adt metrics output data into database '''

    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db
                          --retry
                          --database-url=sqlite:///./csvdb
                          -i "barcode_id"
                          -i "sample"
                          --table=adt_metrics
                 > %(outfile)s
              '''

    P.run(statement)


@follows(load_merged_qcmetrics, load_merged_scrublet,
         load_merged_demux,
         load_channels, load_combatids, load_merged_adt)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("final.load")
def final(outfile):
    ''' '''

    dbh = connect()

    statement = '''CREATE VIEW final AS
                    SELECT qc.BARCODE barcode, qc.sample sequencing_id, qc.ngenes,
                              qc.total_UMI, qc.pct_mitochondrial, qc.pct_ribosomal, qc.pct_immunoglobin,
                              qc.pct_hemoglobin, qc.pct_neutrophil, qc.pct_platelet, qc.pct_endothelial, qc.pct_apoptotic,
                              qc.pct_neutrophil_shortlisted, qc.cellranger_filter, qc.emptydrops_filter,
                              qc.mitoribo_ratio, qc.barcode_id,
                           scrub.scrub_doublet_scores, scrub.scrub_predicted_doublets,
                           adt.nfeatures_ADT, adt.total_UMI_ADT, adt.median_UMI_ADT, adt.log2FeaturesPerUMI,
                           channels.gPlex gplex, channels.Pool pool, channels.Channel channel,
                           cids.PBMCs, cids.COMBAT_ID_Time,
                           cm.COMBAT_ID, cm.scRNASeq_sample_ID 
                    FROM gex_qcmetrics qc
                    LEFT JOIN gex_scrublet scrub
                    ON qc.barcode_id = scrub.barcode_id
                    LEFT JOIN gex_demux demux
                    ON qc.barcode_id = demux.barcode_id
                    LEFT JOIN channels
                    ON qc.sample = channels.gPlex
                    LEFT JOIN combatids cids
                    ON demux.vireo_channel = cids.baseID
                       AND channels.Pool = cids.Pool
                    LEFT JOIN clinical_metadata cm
                    ON cids.PBMCs = cm.scRNASeq_sample_ID
                    LEFT JOIN adt_metrics adt
                    ON qc.barcode_id = adt.barcode_id'''

    cc = database.executewait(dbh, statement, retries=5)

    cc.close()

    iotools.touch_file(outfile)


def full():
    pass

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
