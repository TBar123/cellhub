'''
===============
Pipeline celldb
===============


Overview
========

This pipeline uploads the outputs from the upstream single-cell preprocessing
steps into a SQLite database.

Usage
=====

See :ref:`` and :ref:`` for general information on how to use cgat
pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.yml` file.

Default configuration files can be generated by executing:
   cellhub celldb config

Input files
-----------

The pipeline requires the output of ??. Each task of the upstream
pipeline generates a tsv configured file.

Dependencies
------------

Pipeline output
===============

The pipeline returns an SQLite populated database of metadata and
quality features that aid the selection of 'good' cells from 'bad' cells.

Currently the following tables are generated:
* metadata


Code
====

'''

from ruffus import *

import sys
import os
import re
import sqlite3
import pandas as pd

import cgatcore.experiment as E
from cgatcore import pipeline as P
import cgatcore.iotools as iotools
import cgatcore.database as database


# Load options from the config file

PARAMS = P.get_parameters(
    ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
     "../pipeline.yml",
     "pipeline.yml"])


def connect():
    '''connect to database.
    Use this method to connect to additional databases.
    Returns a database connection.
    '''

    dbh = database.connect(url=PARAMS["database_url"])

    return dbh

def connect_virtual_table():
    '''connect to database for creating virtual table.
    Use this method to connect to additional databases.
    Returns a database connection.
    '''

    cursor = database.apsw_connect(dbname="csvdb")

    return cursor


@transform(PARAMS['data_sequencing_info'],
           regex("\S+/(\S+).tsv"),
           r"\1.tsv")
def preprocess_metadata_sequencing(infile, outfile):
    '''Process metadata from sequencig core because it contains the column Index'''

    df = pd.read_table(infile)
    df.columns = df.columns.str.replace(' ','_')
    df.rename(columns={'Index':'SampleIndex'}, inplace=True)
    df.to_csv(outfile, sep='\t', index=False)


@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform(preprocess_metadata_sequencing,
           suffix(".tsv"),
           r"\1.load")
def load_metadata_sequencing(infile, outfile):
    '''load metadata of sequencing data into database '''

    to_cluster = True

    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db  
                          --retry  
                          --database-url=sqlite:///./csvdb 
                          -i "Sequencing_ID" 
                          --table=metadata_sequencing 
                 > %(outfile)s
              '''

    P.run(statement)

@follows(load_metadata_sequencing)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@files(PARAMS['data_lookup_combatid'],
       r"combatid_lookup.load")
def load_combatid_lookup(infile, outfile):
    '''load lookup table of combat ids into database '''

    to_cluster = True

    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db  
                          --retry  
                          --database-url=sqlite:///./csvdb 
                          -i "Pool" 
                          -i "Sequencing_ID"
                          --table=combatid_lookup
                 > %(outfile)s
              '''

    P.run(statement)


@follows(load_combatid_lookup)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@files(PARAMS['data_lookup_pool'],
       "channel_lookup.load")
def load_channel_lookup(infile, outfile):
    '''load lookup table of channel ids into database '''

    statement='''cat %(infile)s
                 | sed 's/Sequencing.ID/Sequencing_ID/g'
                 | python -m cgatcore.csv2db  
                          --retry  
                          --database-url=sqlite:///./csvdb 
                          -i "Pool" 
                          --table=channel_lookup
                 > %(outfile)s
              '''

    P.run(statement)


@follows(load_channel_lookup)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@files(PARAMS['data_patient_metadata'],
       "metadata_patient.load")
def load_metadata_samples(infile, outfile):
    '''load metadata of patients into database '''


    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db  
                          --retry  
                          --database-url=sqlite:///./csvdb 
                           -i "COMBATID" 
                          --table=metadata_patient
                 > %(outfile)s
              '''

    P.run(statement)


@merge(os.path.join(PARAMS['data_scrublet'],"*_scrublet.tsv.gz"),
           "scrublet_merge.tsv")
def process_merged_scrublet(infiles, outfile):
    '''Process concatenate data together from scrublet'''

    li = []

    for fname in infiles:
        df = pd.read_table(fname, index_col=None)
        #df['barcode_id'] = df["BARCODE"].str.replace("-1", "")
        df['barcode_id'] = df['BARCODE'] + "-" + df['sample']
        li.append(df)

    frame = pd.concat(li, axis=0, ignore_index=True)

    frame.to_csv(outfile, sep='\t', index=False)



@follows(load_metadata_samples)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform(process_merged_scrublet,
           suffix(".tsv"),
           ".load")
def load_merged_scrublet(infile, outfile):
    '''load scrublet output data into database '''


    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db  
                          --retry  
                          --database-url=sqlite:///./csvdb 
                           -i "barcode_id" 
                          --table=gex_scrublet
                 > %(outfile)s
              '''

    P.run(statement)


@merge(os.path.join(PARAMS['data_qcmetrics'],"*_qcmetrics.tsv.gz"),
           "qcmetrics_merge.tsv")
def process_merged_qcmetrics(infiles, outfile):
    '''Process concatenate data together from qcmetrics'''

    li = []

    for fname in infiles:
        df = pd.read_table(fname, index_col=None)
        #df['barcode_id'] = df["BARCODE"].str.replace("-1", "")
        df['barcode_id'] = df['BARCODE'] + "-" + df['sample']
        li.append(df)

    frame = pd.concat(li, axis=0, ignore_index=True)

    frame.to_csv(outfile, sep='\t', index=False)



@follows(load_merged_scrublet)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform(process_merged_qcmetrics,
           suffix(".tsv"),
           ".load")
def load_merged_qcmetrics(infile, outfile):
    '''load qcmetrics output data into database '''

    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db  
                          --retry  
                          --database-url=sqlite:///./csvdb 
                          -i "barcode_id" 
                          -i "sample"
                          --table=gex_qcmetrics
                 > %(outfile)s
              '''

    P.run(statement)


@merge(os.path.join(PARAMS['data_demux'],"*/*_SingleCellMetadata_demultiplexing_results.tsv.gz"),
       "demux_merged.tsv")
def process_merged_demux(infiles, outfile):
    '''Process concatenate data together from qcmetrics'''

    li = []

    for fname in infiles:
        name = os.path.basename(fname).replace("_SingleCellMetadata_demultiplexing_results.tsv.gz","")
        df = pd.read_table(fname, index_col=None)
        df.columns = ['barcode','demuxlet','demuxletV2', 'vireo', 'vireounknown']
        #df['barcode_id'] = df["barcode"].str.replace("-1", "")
        df['barcode_id'] = df['barcode'] + "-" + name
        df['sample'] = name
        li.append(df)

    frame = pd.concat(li, axis=0, ignore_index=True)

    frame.to_csv(outfile, sep='\t', index=False)


@follows(load_merged_qcmetrics)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@transform(process_merged_demux,
           suffix(".tsv"),
           ".load")
def load_merged_demux(infile, outfile):
    ''' '''

    statement='''cat %(infile)s
                 | python -m cgatcore.csv2db  
                          --retry  
                          --database-url=sqlite:///./csvdb 
                           -i "barcode_id" 
                          --table=gex_demux
                 > %(outfile)s
              '''

    P.run(statement)


@follows(load_merged_demux, load_combatid_lookup,
         load_channel_lookup)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("merge_lookups.load")
def merge_lookups(outfile):
    ''' '''

    dbh = connect()

    statement1 = '''DROP TABLE IF EXISTS merged_lookup;'''
    statement2 = '''CREATE TABLE merged_lookup
                   AS 
                   SELECT * FROM
                   combatid_lookup
                   LEFT JOIN channel_lookup
                   ON combatid_lookup.Pool = channel_lookup.Pool;'''

    cc = database.executewait(dbh, statement1, retries=5)
    cc = database.executewait(dbh, statement2, retries=5)

    cc.close()

    iotools.touch_file(outfile)



@follows(load_merged_qcmetrics, load_merged_scrublet,
         merge_lookups)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("final.load")
def final(outfile):
    ''' '''

    dbh = connect()

    statement = '''CREATE VIEW final AS SELECT * FROM gex_qcmetrics
                    LEFT JOIN gex_scrublet
                    ON gex_qcmetrics.barcode_id = gex_scrublet.barcode_id
                    LEFT JOIN gex_demux
                    ON gex_qcmetrics.barcode_id = gex_demux.barcode_id
                    LEFT JOIN metadata_sequencing
                    ON metadata_sequencing.Sequencing_ID = gex_qcmetrics.sample
                    LEFT JOIN merged_lookup
                    ON merged_lookup.Sequencing_ID = metadata_sequencing.Sequencing_ID;'''

    cc = database.executewait(dbh, statement, retries=5)

    cc.close()

    iotools.touch_file(outfile)


def full():
    pass

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
