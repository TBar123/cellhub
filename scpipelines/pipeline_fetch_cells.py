"""===========================
Pipeline fetch cells
===========================

:Author: Sansom lab
:Release: $Id$
:Date: |today|
:Tags: Python

Overview
========

This pipeline fetches a given set of cells from market matrices or
loom files into a single market matrix file.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.yml` file.

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_fetch_cells.py config


Input files
-----------

There are two inputs, both specified in the pipeline.yml file.

(1) A map of cell barcodes to matrix identifiers

This file should be a gzipped tsv file with two columns. Column
1 should have the title "barcode" and contain the cell barcodes.
Column 2 should have the title "matrix_id" and contain a matrix
identifier

barcode     matrix_id
ACCCATCG    channel_1
ATTCATCG    channel_1
AGGCATCG    channel_2
TCCCATCG    channel_2
gCCCATCG    channel_3


(2) A table containing the matrix identifiers, locations and types.

This file should be a tsv file with three columns:

matrix_id    matrix_dir                    matrix_type
channel_1    /full/path/channel_1_matrix   mm
channel_2    /full/path/channel_2_matrix   mm
channel_3    /full/path/channel_3_matrix   mm

The matrix type should be specified as either "mm" for market
matrix format or "loom" for the loom file format.


Dependencies
------------

This pipeline requires:



Pipeline output
===============

The pipeline outputs a folder containing a single market matrix
that contains the requested cells.

"""

import os
import sys
import gzip
from shutil import copyfile
import hashlib
from pathlib import Path
import pandas as pd
from ruffus import *
from cgatcore import pipeline as P
import cgatcore.iotools as IOTools

# -------------------------- < parse parameters > --------------------------- #

# load options from the config file
PARAMS = P.get_parameters(
    ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
     "../pipeline.yml",
     "pipeline.yml"])


# set the location of the tenx code directory
if "cellhub_dir" not in PARAMS.keys():
    PARAMS["cellhub_dir"] = Path(__file__).parents[1]
else:
    raise ValueError("Could not set the location of the "
                     "cellhub code directory")


# ----------------------- < pipeline configuration > ------------------------ #

# handle pipeline configuration
if len(sys.argv) > 1:
        if(sys.argv[1] == "config") and __name__ == "__main__":
                    sys.exit(P.main(sys.argv))


# ------------------------------ < functions > ------------------------------ #

# adapted from:
# https://stackoverflow.com/questions/3431825/
# generating-an-md5-checksum-of-a-file

def md5gz(fname):

    hash_md5 = hashlib.md5()
    with gzip.open(fname, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()


# ########################################################################### #
# ############################# pipeline tasks ############################## #
# ########################################################################### #

@follows(mkdir("cell.info.dir"))
@files(PARAMS["celldb"],
       "cell.info.dir/cell.table.sentinel")
def fetch_cell_table(infile, outfile):
    '''Fetch the table of the user's desired cells from the database
       effectively, cell-metadata tsv table.
    '''

    cell_table = outfile.replace(".sentinel", ".tsv.gz")

    if(PARAMS["sample"]=="all"):
        sample = ""
    else:
        take = int(PARAMS["sample"]) + 1
        sample = '''| body shuf | awk 'NR <= %(take)s' ''' % locals()

    statement ='''body() {
                   IFS= read -r header;
                   printf '%%s\\n' "$header";
                  "$@";
                  };

              sqlite3 -header %(infile)s -separator $'\\t' '%(query)s'
                  %(sample)s
                  | gzip -c
                  > %(cell_table)s'''

    P.run(statement)
    IOTools.touch_file(outfile)

def matrix_subset_jobs():
    '''Generate a list of subsets from the cell metadata'''

    cell_tab = "cell.info.dir/cell.table.tsv.gz"

    if os.path.exists(cell_tab):

        global cells

        cells = pd.read_csv(cell_tab, sep="\t")

        matrix_ids = [x for x in cells[PARAMS['matrix_name']].unique()]

        for matrix_id in matrix_ids:

            matrix_subset_dir = os.path.join("matrix.subsets.dir", matrix_id)

            matrix_subset_barcodes = os.path.join(matrix_subset_dir,
                                                  "cells_to_extract.txt.gz")

            yield [None, matrix_subset_barcodes]


@follows(fetch_cell_table, mkdir("matrix.subsets.dir"))
@files(matrix_subset_jobs)
def setupSubsetJobs(infile, outfile):
    '''Setup the folders for the subsetting jobs'''

    # cells = pd.read_csv("cell.info.dir/cell.table.tsv.gz", sep="\t")

    matrix_subset_dir = os.path.dirname(outfile)
    matrix_id = os.path.basename(Path(outfile).parent)

    os.mkdir(matrix_subset_dir)

    cell_subset = cells["BARCODE"][cells[PARAMS["matrix_name"]] == matrix_id]

    cell_subset.to_csv(outfile,
                       header=False,
                       index=False,
                       compression="gzip")

@transform(setupSubsetJobs,
           regex(r"matrix.subsets.dir/(.*)/cells_to_extract.txt.gz"),
           r"matrix.subsets.dir/\1/matrix.mtx.gz")
def cellSubsets(infile, outfile):
    '''Extract a given subset of cells from a matrix
    if more than a feature modadility, the extrac_cells.R
    function will generate modality specific datasets'''

    to_cluster = True

    matrix_id = os.path.basename(Path(outfile).parent)
    outdir = os.path.dirname(infile)

    if PARAMS['matrix_suffix'] != 'none':
        matrix_id = matrix_id + str(PARAMS['matrix_suffix'])

    if ("G" in PARAMS["resources_memory"] or
        "M" in PARAMS["resources_memory"] ):
        job_memory = PARAMS["resources_memory"]

    log_file = outfile.replace(".mtx.gz", ".extract_cells.log")

    statement = '''Rscript %(cellhub_dir)s/R/extract_cells.R
                   --cells=%(infile)s
                   --matrixdir=%(matrix_dir)s
                   --matrixid=%(matrix_id)s
                   --matrixtype=%(matrix_type)s
                   --outtype=%(matrix_out_type)s
                   --outdir=%(outdir)s
                   &> %(log_file)s
                '''

    P.run(statement)


@follows(mkdir("output.dir"))
@merge(cellSubsets,
       "output.dir/matrix.mtx.gz")
def mergeSubsets(infiles, outfile):
    '''merge the market matrix files into a single matrix'''

    infiles_mods = []
    # Check if more than one feature modality
    for infile in infiles:
        sampledir = Path(infile).parent
        dirs = [sd[0] for sd in os.walk(sampledir)]
        if len(dirs) > 1:
            dirs = dirs[1:len(dirs)]
            for moddir in dirs:
                inputmat = os.path.join(moddir, "matrix.mtx.gz")
                if os.path.exists(inputmat):
                    infiles_mods.append(inputmat)
                    break
    
    modality = {}
    
    if len(infiles) == len(infiles_mods):
        modality["adt"] = infiles_mods

    modality["gex"] = infiles

    for k in modality.keys():
        print(k)

        # get the dimensions of all the market matrix files
        ncells = 0
        mtx_specs = {}
        mtx_encoding = "us-ascii"
        
        infiles_mod = modality[k]

        for subset in infiles_mod:

            outmainfile = outfile
            matrix_id = os.path.basename(Path(subset).parent)

            if k != "gex":
                matrix_id = os.path.basename(Path(Path(subset).parent).parent)
                outdir = os.path.join(os.path.dirname(outfile), k)
                if not os.path.exists(outdir):
                    os.mkdir(outdir)
                outmainfile = os.path.join(outdir, os.path.basename(outfile))

            with gzip.open(subset, "r") as mtx:
                for i, line in enumerate(mtx, 1):
                    if i == 1:
                        mtx_header = line  # .decode("us-ascii").strip()
                    if i == 2:
                        line_str = line.decode(mtx_encoding)
                        nrow, ncol, nnonzero = line_str.strip().split(" ")
                        mtx_specs[matrix_id] = {"nrow": int(nrow),
                                                "ncol": int(ncol),
                                                "nnonzero": int(nnonzero),
                                                "mtx_file": subset}
                        break

        matrices_to_merge = tuple(mtx_specs.keys())

        mtx_outfile = outmainfile[:-len(".gz")]
        barcodes_outfile = os.path.join(os.path.dirname(outmainfile),
                                        "barcodes.tsv")

        features_outfile = os.path.join(os.path.dirname(outmainfile),
                                        "features.tsv.gz")

        if os.path.exists(mtx_outfile) or \
           os.path.exists(mtx_outfile + ".gz"):
            raise ValueError("mtx outfile already exists")

        if os.path.exists(barcodes_outfile) or \
           os.path.exists(barcodes_outfile + ".gz"):
            raise ValueError("barcodes outfile already exists")

        if os.path.exists(features_outfile):
            raise ValueError("features outfile already exists")

        # construct the header of the market matrix file.
        nrows = []
        total_ncol = 0
        total_nnonzero = 0

        for matrix_id in matrices_to_merge:
            nrows.append(mtx_specs[matrix_id]["nrow"])
            total_ncol += mtx_specs[matrix_id]["ncol"]
            total_nnonzero += mtx_specs[matrix_id]["nnonzero"]

        if len(set(nrows)) > 1:
            raise ValueError("the input matrices have different numbers of rows!")

        out_spec = " ".join([str(nrows[0]),
                             str(total_ncol),
                             str(total_nnonzero)]) + "\n"

        # write the header of the market matrix file
        with open(mtx_outfile, "wb") as out:
            out.write(mtx_header)
            out.write(out_spec.encode(mtx_encoding))

        column_offset = 0

        feature_file_checksums = []

        statement = ""

        for matrix_id in matrices_to_merge:

            mtx_file = mtx_specs[matrix_id]["mtx_file"]

            barcodes_file = os.path.join(os.path.dirname(mtx_file),
                                         "barcodes.tsv.gz")

            # append the matrix values, offsetting the column index
            statement += '''zcat %(mtx_file)s
                            | awk 'NR>2{print $1,$2 + %(column_offset)s,$3}'
                            >> %(mtx_outfile)s;
                        ''' % locals()

            # append the barcodes, adding the matrix identifier
            statement += '''zcat %(barcodes_file)s
                            | awk '{print $1"-1-%(matrix_id)s"}'
                            >> %(barcodes_outfile)s;
                        ''' % locals()


            # increase the column offset by the number of appended columns
            column_offset += mtx_specs[matrix_id]["ncol"]

            features_file = os.path.join(os.path.dirname(mtx_file),
                                         "features.tsv.gz")

            feature_file_checksums.append(md5gz(features_file))

        if ("G" in PARAMS["resources_memory"] or
            "M" in PARAMS["resources_memory"] ):
            job_memory = PARAMS["resources_memory"]

        # run the job.
        P.run(statement)

        # check that all of the subsets have identical features.
        if len(set(feature_file_checksums)) != 1:
            raise ValueError("The matrices have different features")
        else:
            print("The matrices have the same features")

        if ("G" in PARAMS["resources_memory"] or
            "M" in PARAMS["resources_memory"] ):
            job_memory = PARAMS["resources_memory"]

        # compress the outfiles
        statement = '''gzip %(mtx_outfile)s;
                       gzip %(barcodes_outfile)s
                    '''

        P.run(statement)

        # copy over the features to the output directory
        copyfile(features_file, features_outfile)

        outfile_sent = outmainfile.replace(".mtx.gz",
                                       ".sentinel")

        if k == "gex":
            IOTools.touch_file(outfile_sent)

@follows(mergeSubsets)
@transform(mergeSubsets,
        regex(r"output.dir/(.*).mtx.gz"),
        r"output.dir/\1_merged_qcmetrics_report.sentinel")
def build_qc_reports(infile, outfile):
    '''This task will run R/build_qc_mapping_report.R,
    It expects three files in the input directory barcodes.tsv.gz,
    features.tsv.gz, and matrix.mtx.gz
    Ouput: creates a _qcmetrics_report.pdf 
    '''

    # Get input directory and id
    sample_name = "merged"
    input_dir = os.path.dirname(infile)

    job_threads = 1
    job_memory = "40G"

    log_file = outfile.replace(".pdf", ".log")

    sampledir = Path(infile).parent
    dirs = [sd[0] for sd in os.walk(sampledir)]
    if len(dirs) > 1:
        dirs = dirs[1:len(dirs)]
        for moddir in dirs:
            inputmat = Path(moddir)
            mod = os.path.basename(inputmat)
            logfile = "qc_mapping_report_" + mod + ".log"
            log_mod_file = os.path.join(moddir, logfile)
            # Formulate and run statement
            statement = '''Rscript %(cellhub_dir)s/R/build_qc_mapping_reports.R
                        --tenxfolder=%(inputmat)s
                        --sample_id=%(mod)s
                        --specie="hg"
                        --outfolder=%(inputmat)s
                        &> %(log_mod_file)s
                        '''
            P.run(statement)

    # Formulate and run statement
    statement = '''Rscript %(cellhub_dir)s/R/build_qc_mapping_reports.R
                --tenxfolder=%(input_dir)s
                --sample_id=%(sample_name)s
                --specie="hg"
                --outfolder=%(input_dir)s
                &> %(log_file)s
                '''
    P.run(statement)

    IOTools.touch_file(outfile)


@follows(mkdir("output.subsampled.dir"))
@transform(mergeSubsets,
       regex(r"output.dir/(.*).mtx.gz"),
       r"output.subsampled.dir/\1.sentinel")
def transcriptDownsample(infile, outfile):
    '''
    Down-sample transcripts 
    given a cell-metadata variable
    '''

    outdir = os.path.dirname(outfile)

    agg_matrix_dir = os.path.dirname(infile)

    cellgroup_var = "sample_id"
    downsampling_function = "median"
    cellmetadatafile = "cell.info.dir/cell.table.tsv.gz"

    log_file = outfile.replace(".sentinel",
                               ".log")

    if ("G" in PARAMS["resources_memory"] or
        "M" in PARAMS["resources_memory"] ):
        job_memory = PARAMS["resources_memory"]

    statement = '''Rscript %(cellhub_dir)s/R/downSampleTranscripts.R
                   --genexpdir=%(agg_matrix_dir)s
                   --cellmetadata=%(cellmetadatafile)s
                   --cellgroupvar=%(cellgroup_var)s
                   --downsample=%(downsampling_function)s
                   --outdir=%(outdir)s
                   &> %(log_file)s
                '''

    P.run(statement)

    IOTools.touch_file(outfile)

@follows(transcriptDownsample)
@transform(transcriptDownsample,
        regex(r"output.subsampled.dir/(.*).sentinel"),
        r"output.subsampled.dir/\1_subsampled_qcmetrics_report.sentinel")
def build_subsampled_qc_reports(infile, outfile):
    '''This task will run R/build_qc_mapping_report.R,
    It expects three files in the input directory barcodes.tsv.gz,
    features.tsv.gz, and matrix.mtx.gz
    Ouput: creates a _qcmetrics_report.pdf 
    '''

    # Get input directory and id
    sample_name = "subsampled_merged"
    input_dir = os.path.dirname(infile)

    job_threads = 1
    job_memory = "30G"

    log_file = outfile.replace(".pdf", ".log")

    # Formulate and run statement
    statement = '''Rscript %(cellhub_dir)s/R/build_qc_mapping_reports.R
                --tenxfolder=%(input_dir)s
                --sample_id=%(sample_name)s
                --specie="hg"
                --outfolder=%(input_dir)s
                &> %(log_file)s
                '''
    P.run(statement)

    IOTools.touch_file(outfile)



@follows(transcriptDownsample)
@transform(transcriptDownsample,
           regex(r"output.subsampled.dir/(.*).sentinel"),
           add_inputs(fetch_cell_table),
           r"output.subsampled.dir/\1.h5ad")
def exportSubAnnData(infiles, outfile):
    '''
       Export a h5ad anndata matrix for downstream analysis with
       scanpy
    '''

    mtx, obs = infiles
    mtx = mtx.replace(".sentinel", ".mtx.gz")
    mtx_dir = os.path.dirname(mtx)

    outdir = os.path.dirname(outfile)
    matrix_name = os.path.basename(outfile)

    obs_file = obs.replace(".sentinel",".tsv.gz")

    log_file = outfile + ".log"
    job_threads = 1

    if ("G" in PARAMS["resources_memory"] or
        "M" in PARAMS["resources_memory"] ):
        job_memory = PARAMS["resources_memory"]

    statement = '''python %(cellhub_dir)s/python/convert_mm_to_h5ad.py
                          --mtxdir10x=%(mtx_dir)s
                          --obsdata=%(obs_file)s
                          --obstotals=total_UMI
                          --outdir=%(outdir)s
                          --matrixname=%(matrix_name)s
                    > %(log_file)s
                 '''

    P.run(statement)

@follows(mkdir("anndata.dir"))
@transform(mergeSubsets,
           regex(r"output.dir/matrix.mtx.gz"),
           add_inputs(fetch_cell_table),
           r"anndata.dir/matrix.h5ad")
def exportAnnData(infiles, outfile):
    '''
       Export a h5ad anndata matrix for downstream analysis with
       scanpy
    '''

    mtx, obs = infiles
    mtx_dir = os.path.dirname(mtx)

    outdir = os.path.dirname(outfile)
    matrix_name = os.path.basename(outfile)

    obs_file = obs.replace(".sentinel",".tsv.gz")

    log_file = outfile + ".log"
    job_threads = 1

    if ("G" in PARAMS["resources_memory"] or
        "M" in PARAMS["resources_memory"] ):
        job_memory = PARAMS["resources_memory"]

    dirs = [sd[0] for sd in os.walk(mtx_dir)]
    if len(dirs) > 1:
        dirs = dirs[1:len(dirs)]
        for moddir in dirs:
            inputmat = Path(moddir)
            mod = os.path.basename(inputmat)
            out_mod_dir = os.path.join("anndata.dir", mod)
            if not os.path.exists(out_mod_dir):
                os.mkdir(out_mod_dir)
            logfile = os.path.basename(outfile) + "_" + mod + ".log"
            log_mod_file = os.path.join(out_mod_dir, logfile)
            # Formulate and run statement
            statement = '''python %(cellhub_dir)s/python/convert_mm_to_h5ad.py
                                --mtxdir10x=%(inputmat)s
                                --obsdata=%(obs_file)s
                                --obstotals=adt_UMI
                                --outdir=%(out_mod_dir)s
                                --matrixname=%(matrix_name)s
                            > %(log_mod_file)s
                        '''
            P.run(statement)


    statement = '''python %(cellhub_dir)s/python/convert_mm_to_h5ad.py
                          --mtxdir10x=%(mtx_dir)s
                          --obsdata=%(obs_file)s
                          --obstotals=total_UMI
                          --outdir=%(outdir)s
                          --matrixname=%(matrix_name)s
                    > %(log_file)s
                 '''

    P.run(statement)


# ########################################################################### #
# ##################### full target: to run all tasks ####################### #
# ########################################################################### #

@follows(exportAnnData, build_subsampled_qc_reports, build_qc_reports, exportSubAnnData)
def full():
    pass


# ------------------- < ***** end of pipeline **** > ------------------------ #

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
