##############################################################################
#
#   Kennedy Institute of Rheumatology
#
#   $Id$
#
#   Copyright (C) 2018 Stephen Sansom
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################

"""===========================
Pipeline Cellranger
===========================

:Author: Sansom lab
:Release: $Id$
:Date: |today|
:Tags: Python

Overview
========

This pipeline performs the following functions:

* Alignment and quantitation (using cellranger count or cellranger multi)
 
Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
------------

The pipeline requires a configured :file:`pipeline.yml` file.

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_cellranger.py config


Input files
-----------

The pipeline can be run from FASTQ

(A) If starting from FASTQ files:

The pipeline expects a file describing each sample to be present
in a "data.dir" subfolder.

The sample file should contain the path(s) to the output(s) of
"cellranger mkfastq". If multiple sequencing runs were performed,
specify one path per line.

The name of the sample file must follow the a four-part syntax:
"sample_name.ncells.batch.sample"

Where:
1. "sample_name" is a user specified sample name;
2. "ncells" is an integer giving the number of expected cells;
3. "batch" is the sample batch. Libraries sequenced on the same lane(s)
should be assigned the same batch. This is used to filter out cells
which may be exposed to index hopping.
4. ".sample" is a required suffix.

e.g.

$ cat data.dir/donor1_butyrate.1000.1.sample
/gfs/work/ssansom/10x/holm_butyrate/cellranger/data.dir/392850_21
/gfs/work/ssansom/10x/holm_butyrate/cellranger/data.dir/397106_21


Dependencies
------------

This pipeline requires:
* cgat-core: https://github.com/cgat-developers/cgat-core
* cellranger: https://support.10xgenomics.com/single-cell-gene-expression/
#* picard tools (optional): https://broadinstitute.github.io/picard/
#* R & various packages.


Pipeline output
===============

The pipeline returns:
* the output of cellranger multi or cellranger count (if run from FASTQs)
* QC, aggr, dropEst and uploading to sqlite3 DB are run in separate pipelines

The outputs are compatible with pipeline_seurat.py.

Code
====

"""
from ruffus import *
from pathlib import Path
import sys
import os
import glob
import sqlite3
import yaml
import  csv

import cgatcore.experiment as E
from cgatcore import pipeline as P
import cgatcore.iotools as IOTools

# modules for exploring environment
import subprocess
import pkg_resources

import pandas as pd
import numpy as np

# import local pipeline utility functions
from pipeline_utils import templates
from pipeline_utils import resources
from pipeline_utils import TASK



# -------------------------- < parse parameters > --------------------------- #

# load options from the config file
PARAMS = P.get_parameters(
    ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
     "../pipeline.yml",
     "pipeline.yml"])

# set the location of the pipeline code directory
if "code_dir" not in PARAMS.keys():
    PARAMS["code_dir"] = Path(__file__).parents[1]
else:
    raise ValueError("Could not set the location of the pipeline code directory")


# ----------------------- < pipeline configuration > ------------------------ #

# handle pipeline configuration
if len(sys.argv) > 1:
        if(sys.argv[1] == "config") and __name__ == "__main__":
                    sys.exit(P.main(sys.argv))


# ----------------------- < helper functions > ------------------------ #


@files(None, "task.summary.table.tex")
def taskSummary(infile, outfile):
    '''Make a summary of optional tasks that will be run'''

    tasks, run = [], []

    for k,v in PARAMS.items():
        if k.startswith("run_"):
            tasks.append(k[4:])
            run.append(str(v))

    tab = pd.DataFrame(list(zip(tasks,run)),columns=["task","run"])
    print(tab)

    tab.to_latex(buf=outfile, index=False)



# ########################################################################### #
# ######## Read parameters and create config file ############### #
# ########################################################################### #

@active_if(PARAMS["input"] == "mkfastq")
@follows(mkdir("data.dir"))
@originate("config.sentinel")

def makeConfig(outfile):
    
    '''Read parameters from yml file for the whole experiment and save config files as csv.'''

    # check if references exist
    if PARAMS["run_gene-expression"]:
        gexref = PARAMS["gene-expression_reference"]
        if gexref is None:
            raise ValueError('"gene-expression_reference" parameter not set'
                             ' in file "pipeline.yml"')

        if not os.path.exists(gexref):
            raise ValueError('The specified "gene-expression_reference"'
                             ' file does not exist')
    else:
        pass

    if PARAMS["run_feature"]:
        featureref = PARAMS["feature_reference"]
        if featureref is None:
            raise ValueError('"feature_reference" parameter not set'
                             ' in file "pipeline.yml"')

        if not os.path.exists(featureref):
            raise ValueError('The specified "feature_reference"'
                             ' file does not exist')
    else:
        pass

    if PARAMS["run_vdj"]:
        vdjref = PARAMS["vdj_reference"]
        if vdjref is None:
            raise ValueError('"vdj_reference" parameter not set'
                             ' in file "pipeline.yml"')

        if not os.path.exists(vdjref):
            raise ValueError('The specified "vdj_reference"'
                             ' file does not exist')
    else:
        pass


    # read parameters for gex
    section, param = [], []
    
    if PARAMS["run_gene-expression"]:
        for k,v in PARAMS.items():
            if k.startswith("gene-expression_"):
                if v is not None:
                    section.append(k[16:]) 
                    param.append(str(v))

        df_gex = pd.DataFrame(list(zip(section,param)),columns=["[gene-expression]",""])
    else:
        pass

    # read parameters for feature
    section, param = [], []

    if PARAMS["run_feature"]:
        for k,v in PARAMS.items():
            if k.startswith("feature_"):
                if v is not None:
                    section.append(k[8:])
                    param.append(str(v))

        df_feature = pd.DataFrame(list(zip(section,param)),columns=["[feature]",""])
    else:
        pass    

    # read parameters for vdj
    section, param = [], []

    if PARAMS["run_vdj"]:
        for k,v in PARAMS.items():
            if k.startswith("vdj_"):
                if v is not None:
                    section.append(k[4:])
                    param.append(str(v))

        df_vdj = pd.DataFrame(list(zip(section,param)),columns=["[vdj]",""])
    else:
        pass

    # read parameters for libraries:
    lib_params = PARAMS["libraries"]
    samples = list(lib_params.keys())    

    for i in samples:
        
        # Save subsections of parameters in config files specific for each sample (data.dir/sample01.csv data.dir/sample02.csv etc)     
        filename = "data.dir/" + i + ".csv"
        libsample_params = PARAMS["libraries_" + i]
        lib_df = pd.DataFrame(libsample_params)

        lib_columns = list(lib_df)

        smp_df = pd.DataFrame()
        for i in lib_columns:
            tmp = lib_df[i].str.split(',', expand=True)
            smp_df = smp_df.append(tmp.T)

            # filter out gex rows from libraries table if run_gene-expression = false
            mask = smp_df.feature_types == 'Gene Expression'
            if PARAMS["run_gene-expression"]:
                df_filt = smp_df
            else:
                df_filt = smp_df[~mask]

            # filter out feature rows from libraries table if run_feature = false
            mask = df_filt.feature_types == 'Antibody Capture'
            if PARAMS["run_feature"]:
                df_filt = df_filt
            else:
                df_filt = df_filt[~mask]

            # filter out vdj rows from libraries table if run_vdj = false
            mask = df_filt.feature_types == 'VDJ-B'
            if PARAMS["run_vdj"]:
                df_filt = df_filt
            else:
                df_filt = df_filt[~mask]
         

        # but I need to add different headers for each subsection, so I stream each table individually.
        with open(filename, 'a') as csv_stream:

            if PARAMS["run_gene-expression"]:            
                csv_stream.write('[gene-expression]\n')
                df_gex.to_csv(csv_stream, header=False, index=False)
                csv_stream.write('\n')
            else:
                pass

            if PARAMS["run_feature"]:
                csv_stream.write('[feature]\n')
                df_feature.to_csv(csv_stream, header=False, index=False)
                csv_stream.write('\n')    
            else:
                pass
           
            if PARAMS["run_vdj"]: 
                csv_stream.write('[vdj]\n')
                df_vdj.to_csv(csv_stream, header=False, index=False)
                csv_stream.write('\n')
            else:
                pass

            csv_stream.write('[libraries]\n')
            df_filt.to_csv(csv_stream, header=True, index=False)
            csv_stream.write('\n')

# ########################################################################### #
# ############################ run cellranger multi ######################### #
# ########################################################################### #

@active_if(PARAMS["input"] == "mkfastq")
@follows(makeConfig)
@transform("data.dir/*.csv",
           regex(r".*/([^.]*).*.csv"),
           r"\1-check/cellranger.multi.sentinel")

#@files(None,
#       "input.check.sentinel")

def cellrangerMulti(infile, outfile):
    '''
    Execute the cellranger multi pipleline for first sample.
    '''

    # read id_tag from file name
    config_path = os.path.abspath(infile)
    sample_basename = os.path.basename(infile)
    sample_name_sections = sample_basename.split(".")
    id_tag = sample_name_sections[0]
    log_file = id_tag + ".log"

    #set the maximum number of jobs for cellranger
    max_jobs = PARAMS["cellranger_maxjobs"]

    ## send one job script to slurm queue which arranges cellranger run
    ## hard-coded to ensure enough resources
    job_threads = 24
    job_memory = "24G"

# this statement is to run in slurm mode
    statement = (
	'''cellranger multi 
	    	    --id %(id_tag)s 
                    --csv=%(config_path)s
                    --jobmode=slurm
                    --maxjobs=%(max_jobs)s
		    --nopreflight          
            &> %(log_file)s
        ''')

    P.run(statement)

    IOTools.touch_file(outfile)


# ---------------------------------------------------
# Generic pipeline tasks

@follows(makeConfig,cellrangerMulti)
def full():
    '''
    Run the full pipeline.
    '''
    pass


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
