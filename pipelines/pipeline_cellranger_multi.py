'''
=========================
Pipeline Cellranger Multi
=========================


Overview
========

This pipeline performs the following functions:

* Alignment and quantitation (using cellranger count or cellranger multi)

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline_cellranger_multi.yml` file.

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_cellranger_multi.py config


Dependencies
------------

This pipeline requires:
* cgat-core: https://github.com/cgat-developers/cgat-core
* cellranger: https://support.10xgenomics.com/single-cell-gene-expression/


Pipeline output
===============

The pipeline returns:
* the output of cellranger multi

Code
====

'''

from ruffus import *
from pathlib import Path
import sys
import os
import glob
import sqlite3
import yaml
import  csv

import cgatcore.experiment as E
from cgatcore import pipeline as P
import cgatcore.iotools as IOTools

# modules for exploring environment
import subprocess
import pkg_resources

import pandas as pd
import numpy as np

# import local pipeline utility functions
from tasks import templates
from tasks import resources
from tasks import TASK



# -------------------------- < parse parameters > --------------------------- #

# load options from the config file
PARAMS = P.get_parameters(
    ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
     "../pipeline.yml",
     "pipeline.yml"])

# set the location of the pipeline code directory
if "code_dir" not in PARAMS.keys():
    PARAMS["code_dir"] = Path(__file__).parents[1]
else:
    if PARAMS["code_dir"] != Path(__file__).parents[1]:
        raise ValueError("Could not set the location of "
                         "the pipeline code directory")

# ----------------------- < pipeline configuration > ------------------------ #

# handle pipeline configuration
if len(sys.argv) > 1:
        if(sys.argv[1] == "config") and __name__ == "__main__":
                    sys.exit(P.main(sys.argv))


# ----------------------- < helper functions > ------------------------ #


@files(None, "task.summary.table.tex")
def taskSummary(infile, outfile):
    '''Make a summary of optional tasks that will be run'''

    tasks, run = [], []

    for k,v in PARAMS.items():
        if k.startswith("run_"):
            tasks.append(k[4:])
            run.append(str(v))

    tab = pd.DataFrame(list(zip(tasks,run)),columns=["task","run"])
    print(tab)

    tab.to_latex(buf=outfile, index=False)



# ########################################################################### #
# ################ Read parameters and create config file ################### #
# ########################################################################### #

@active_if(PARAMS["input"] == "mkfastq")
@follows(mkdir("data.dir"))
@originate("config.sentinel")

def makeConfig(outfile):
    '''Read parameters from yml file for the whole experiment and save config files as csv.'''

    # check if references exist
    if PARAMS["run_gene-expression"]:
        gexref = PARAMS["gene-expression_reference"]
        if gexref is None:
            raise ValueError('"gene-expression_reference" parameter not set'
                             ' in file "pipeline.yml"')

        if not os.path.exists(gexref):
            raise ValueError('The specified "gene-expression_reference"'
                             ' file does not exist')
    else:
        pass

    if PARAMS["run_feature"]:
        featureref = PARAMS["feature_reference"]
        if featureref is None:
            raise ValueError('"feature_reference" parameter not set'
                             ' in file "pipeline.yml"')

        if not os.path.exists(featureref):
            raise ValueError('The specified "feature_reference"'
                             ' file does not exist')
    else:
        pass

    if PARAMS["run_vdj"]:
        vdjref = PARAMS["vdj_reference"]
        if vdjref is None:
            raise ValueError('"vdj_reference" parameter not set'
                             ' in file "pipeline.yml"')

        if not os.path.exists(vdjref):
            raise ValueError('The specified "vdj_reference"'
                             ' file does not exist')
    else:
        pass


    # read parameters for gex
    section, param = [], []

    if PARAMS["run_gene-expression"]:
        for k,v in PARAMS.items():
            if k.startswith("gene-expression_"):
                if v is not None:
                    section.append(k[16:])
                    param.append(str(v))

        df_gex = pd.DataFrame(list(zip(section,param)),columns=["[gene-expression]",""])
    else:
        pass

    # read parameters for feature
    section, param = [], []

    if PARAMS["run_feature"]:
        for k,v in PARAMS.items():
            if k.startswith("feature_"):
                if v is not None:
                    section.append(k[8:])
                    param.append(str(v))

        df_feature = pd.DataFrame(list(zip(section,param)),columns=["[feature]",""])
    else:
        pass

    # read parameters for vdj
    section, param = [], []

    if PARAMS["run_vdj"]:
        for k,v in PARAMS.items():
            if k.startswith("vdj_"):
                if v is not None:
                    section.append(k[4:])
                    param.append(str(v))

        df_vdj = pd.DataFrame(list(zip(section,param)),columns=["[vdj]",""])
    else:
        pass

    # read parameters for libraries:
    lib_params = PARAMS["libraries"]
    samples = list(lib_params.keys())

    for i in samples:

        # Save subsections of parameters in config files specific for each sample
        # (data.dir/sample01.csv data.dir/sample02.csv etc)
        libsample_params = PARAMS["libraries_" + i]
        sample_name = libsample_params["name"]
        filename = "data.dir/" + i + "_"+ sample_name + ".csv"
        lib_df = pd.DataFrame(libsample_params)
        lib_df.drop('name', axis=1, inplace=True)

        lib_columns = list(lib_df)

        smp_df = pd.DataFrame()
        for i in lib_columns:
            tmp = lib_df[i].str.split(',', expand=True)
            smp_df = smp_df.append(tmp.T)

            # filter out gex rows from libraries table if run_gene-expression = false
            mask = smp_df.feature_types == 'Gene Expression'
            if PARAMS["run_gene-expression"]:
                df_filt = smp_df
            else:
                df_filt = smp_df[~mask]

            # filter out feature rows from libraries table if run_feature = false
            mask = df_filt.feature_types == 'Antibody Capture'
            if PARAMS["run_feature"]:
                df_filt = df_filt
            else:
                df_filt = df_filt[~mask]

            # filter out vdj rows from libraries table if run_vdj = false
            mask = df_filt.feature_types == 'VDJ-B'
            if PARAMS["run_vdj"]:
                df_filt = df_filt
            else:
                df_filt = df_filt[~mask]


        # but I need to add different headers for each subsection, so I stream each table individually.
        with open(filename, 'a') as csv_stream:

            if PARAMS["run_gene-expression"]:
                csv_stream.write('[gene-expression]\n')
                df_gex.to_csv(csv_stream, header=False, index=False)
                csv_stream.write('\n')
            else:
                pass

            if PARAMS["run_feature"]:
                csv_stream.write('[feature]\n')
                df_feature.to_csv(csv_stream, header=False, index=False)
                csv_stream.write('\n')
            else:
                pass

            if PARAMS["run_vdj"]:
                csv_stream.write('[vdj]\n')
                df_vdj.to_csv(csv_stream, header=False, index=False)
                csv_stream.write('\n')
            else:
                pass

            csv_stream.write('[libraries]\n')
            df_filt.to_csv(csv_stream, header=True, index=False)
            csv_stream.write('\n')

# ########################################################################### #
# ############################ run cellranger multi ######################### #
# ########################################################################### #

@follows(makeConfig)
@transform("data.dir/*.csv",
           regex(r".*/([^.]*).*.csv"),
           r"\1-cellranger.multi.sentinel")

def cellrangerMulti(infile, outfile):
    '''
    Execute the cellranger multi pipleline for first sample.
    '''

    # read id_tag from file name
    config_path = os.path.abspath(infile)
    sample_basename = os.path.basename(infile)
    sample_name_sections = sample_basename.split(".")
    id_tag = sample_name_sections[0]
    log_file = id_tag + ".log"

    #set the maximum number of jobs for cellranger
    max_jobs = PARAMS["cellranger_maxjobs"]

    ## send one job script to slurm queue which arranges cellranger run
    ## hard-coded to ensure enough resources
    job_threads = 24
    job_memory = "24G"

# this statement is to run in slurm mode
    statement = (
	'''cellranger multi
	    	    --id %(id_tag)s
                    --csv=%(config_path)s
                    --jobmode=slurm
                    --maxjobs=%(max_jobs)s
		    --nopreflight
            &> %(log_file)s
        ''')

    P.run(statement)

    IOTools.touch_file(outfile)

@follows(makeConfig)
@merge("data.dir/*.csv",
        "input_samples.tsv")
def makeSampleTable(sample_files, outfile):
    # Build the path to the log file
    log_file = outfile.replace(".tsv", ".log")

    sample_names = []

    for s in sample_files:
        sample_name = os.path.basename(s)
        sample_names.append(sample_name)

    samples = '///'.join(sample_names)

    job_threads = 2
    job_memory = "2000M"

    statement = '''Rscript %(code_dir)s/R/sampleName2metadatatable.R
                    --outfile=%(outfile)s
                    --samplefiles=%(samples)s
                    &> %(log_file)s
                '''
    P.run(statement)

    IOTools.touch_file(outfile + ".sentinel")

#
# ---------------------------------------------------
# Generic pipeline tasks

@follows(makeConfig,cellrangerMulti,makeSampleTable)
def full():
    '''
    Run the full pipeline.
    '''
    pass


def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
