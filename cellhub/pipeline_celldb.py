'''
===============
Pipeline celldb
===============

Overview
========

This pipeline uploads the outputs from the preprocessing pipelines into a SQLite database. Cell identifiers are joined with sample metadata, qc statistics and other per-cell information in a virtual table called "final" in the database.

Configuration
-------------

The pipeline requires a configured :file:`pipeline_celldb.yml` file.

Default configuration files can be generated by executing:
   cellhub celldb config
   
The user must edit the final_sql_query parameter in the configuration file to create the "final" database view appropriately.

Input files
-----------

#. A user-supplied tab-separated sample metadata file (e.g. "samples.tsv") via a path in the pipeline_celldb.yml configuration file. It should have columns for library_id, sample_id as well as any other relevant experimental metadata such as condition, genotype, age, replicate, sex etc.

#. The pipeline requires the outputs of pipeline_cell_qc to be registered on the API

#. Optionally, the pipeline will load the results of pipeline_singleR and pipeline_dehash from the API if these tables are set to "active" in the configuration file.


Pipeline output
---------------

The pipeline returns an SQLite database that contains a "final" view which links cell identifiers with cell QC information, scrublet scores, user-provided metadata, cell type predictions (optional) and de-multiplexing assignments (optional). In the database cell identify is encoded by the "barcode" and "library_id" fields which are automatically indexed (as a multi-column index).

Code
====

'''


from ruffus import *

import sys
import os
import re
import sqlite3
import pandas as pd
import numpy as np
import glob

import cgatcore.experiment as E
from cgatcore import pipeline as P
import cgatcore.iotools as iotools
import cgatcore.database as database
import cgatcore.csv2db as csv2db

import cellhub.tasks.control as C
import cellhub.tasks.db as DB
import cellhub.tasks.celldb as celldb

# Override function to collect config files
P.control.write_config_files = C.write_config_files

# ----------------------------- parse parameters ----------------------------- #

# load options from the yml file
parameter_file = C.get_parameter_file(__file__,__name__)
PARAMS = P.get_parameters(parameter_file)

# ----------------------------- helper functions ----------------------------- #

def connect():
    '''Helper function to connect to DB'''
    db_url=PARAMS["database_url"]
    dbhandle = database.connect(url=db_url)
    
    return dbhandle
    
def is_active(param_sub_dict):
    ''' to work around autodoc issue'''
    return param_sub_dict["active"]

# ---------------------------- pipeline functions ---------------------------- #

@follows(mkdir("celldb.dir"))
@originate("celldb.dir/sample.load")
def load_samples(outfile):
    ''' load the sample metadata table '''

    x = PARAMS["table_sample"]

    DB.load(x["name"],
            x["path"],
            db_url=PARAMS["database_url"],
            outfile=outfile)


@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("celldb.dir/gex_qcmetrics.load")
def load_gex_qcmetrics(outfile):
    '''load the gex qcmetrics into the database '''

    x = PARAMS["table_gex_qcmetrics"]

    DB.load(x["name"],
            x["path"],
            db_url=PARAMS["database_url"],
            glob=x["glob"],
            outfile=outfile)


@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("celldb.dir/scrublet.load")
def load_gex_scrublet(outfile):
    '''Load the scrublet scores into database.'''

    x = PARAMS["table_gex_scrublet"]

    DB.load(x["name"],
            x["path"],
            db_url=PARAMS["database_url"],
            glob=x["glob"],
            outfile=outfile)

@active_if(is_active(PARAMS["table_gex_singleR"]))
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("celldb.dir/singleR.load")
def load_singleR(outfile):
    '''Load the singleR predictions into the database.'''

    x = PARAMS["table_gex_singleR"]

    DB.load(x["name"],
            x["path"],
            db_url=PARAMS["database_url"],
            outfile=outfile)


@active_if(is_active(PARAMS["table_gmm_demux"]))
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("celldb.dir/gmm.demux.load")
def load_gmm_demux(outfile):
    '''
    Load the gmm demux dehashing calls into the database.
    '''

    x = PARAMS["table_gmm_demux"]

    DB.load(x["name"],
            x["path"],
            db_url=PARAMS["database_url"],
            glob=x["glob"],
            outfile=outfile)

@active_if(is_active(PARAMS["table_demuxEM"]))
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("celldb.dir/demuxEM.load")
def load_demuxEM(outfile):
    '''load the demuxEM dehashing calls into the database '''

    x = PARAMS["table_demuxEM"]

    DB.load(x["name"],
            x["path"],
            db_url=PARAMS["database_url"],
            glob=x["glob"],
            outfile=outfile)


@follows(load_samples,
         load_gex_qcmetrics,
         load_gex_scrublet,
         load_singleR,
         load_gmm_demux,
         load_demuxEM)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("celldb.dir/final.sentinel")
def final(outfile):
    '''
    Construct a "final" view on the database from which
    the cells can be selected and fetched by
    pipeline_fetch_cells.py
    '''

    viewname = "final"
    dbhandle = connect()

    statement = "DROP VIEW IF EXISTS " + viewname
    cc = database.executewait(dbhandle, statement, retries=20)
    cc.close()
    statement = PARAMS["table_final"]["sql_query"]
    cc = database.executewait(dbhandle, statement, retries=20)
    cc.close()
    
    # drop duplicate barcode and library_id columns
    # requires qlite >= 3.35 !
    
    # columns = DB.getColumnNames(dbhandle, viewname)
    
    # cols_to_drop = [x for x in columns 
    #                 if x.startswith("barcode:") or x.startswith("library_id:")]
    
    # for col in cols_to_drop:
    
    #     statement = 'ALTER VIEW %(viewname)s DROP COLUMN %(col)s' % locals()
    #     cc = database.executewait(dbhandle, statement, retries=20)
    #     cc.close()
    

    iotools.touch_file(outfile)

# --------------------- full target: to run all functions -------------------- #

@follows(final)
def full():
    pass

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
