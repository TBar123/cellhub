'''
===============
Pipeline celldb
===============


Overview
========

This pipeline uploads the outputs from the upstream single-cell preprocessing
steps into a SQLite database.

Usage
=====

See :doc:`Installation</Installation>` and :doc:`Usage</Usage>` for general information on how to use cgat
pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.yml` file.

Default configuration files can be generated by executing:
   cellhub celldb config

Input files
-----------

The pipeline requires the output of the pipelines:
    >> pipeline_cellranger.py : sample/10X-chip-channel x design-metadata
    >> pipeline_qc_metrics.py : barcode/cell x sequencing + mapping metadata
    >> pipeline_ambient_rna.py : gene/feature x sequencing + mapping metadata

pipeline generates a tsv configured file.

Dependencies
------------

Pipeline output
===============

The pipeline returns an SQLite populated database of metadata and
quality features that aid the selection of 'good' cells from 'bad' cells.

Currently the following tables are generated:
* metadata

Code
====

'''

from ruffus import *

import sys
import os
import re
import sqlite3
import pandas as pd
import numpy as np
import glob

import cgatcore.experiment as E
from cgatcore import pipeline as P
import cgatcore.iotools as iotools
import cgatcore.database as database
import cgatcore.csv2db as csv2db

import cellhub.tasks.control as C
import cellhub.tasks.db as DB
import cellhub.tasks.celldb as celldb

# Override function to collect config files
P.control.write_config_files = C.write_config_files

# load options from the yml file
parameter_file = C.get_parameter_file(__file__,__name__)
PARAMS = P.get_parameters(parameter_file)

def connect():

    db_url=PARAMS["database_url"]
    dbhandle = database.connect(url=db_url)
    
    return dbhandle


@follows(mkdir("celldb.dir"))
@originate("celldb.dir/sample.load")
def load_samples(outfile):
    ''' load the sample metadata table '''

    x = PARAMS["table_sample"]

    DB.load(x["name"],
            x["path"],
            db_url=PARAMS["database_url"],
            outfile=outfile)


@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("celldb.dir/gex_qcmetrics.load")
def load_gex_qcmetrics(outfile):
    '''load the gex qcmetrics into the database '''

    x = PARAMS["table_gex_qcmetrics"]

    DB.load(x["name"],
            x["path"],
            db_url=PARAMS["database_url"],
            glob=x["glob"],
            outfile=outfile)


@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("celldb.dir/scrublet.load")
def load_gex_scrublet(outfile):
    '''Load the scrublet scores into database.'''

    x = PARAMS["table_gex_scrublet"]

    DB.load(x["name"],
            x["path"],
            db_url=PARAMS["database_url"],
            glob=x["glob"],
            outfile=outfile)

@active_if(PARAMS["table_gex_singleR"]["active"])
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("celldb.dir/singleR.load")
def load_singleR(outfile):
    '''Load the singleR predictions into the database.'''

    x = PARAMS["table_gex_singleR"]

    DB.load(x["name"],
            x["path"],
            db_url=PARAMS["database_url"],
            outfile=outfile)


@active_if(PARAMS["table_gmm_demux"]["active"])
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("celldb.dir/gmm.demux.load")
def load_gmm_demux(outfile):
    '''
    Load the gmm demux dehashing calls into the database.
    '''

    x = PARAMS["table_gmm_demux"]

    DB.load(x["name"],
            x["path"],
            db_url=PARAMS["database_url"],
            glob=x["glob"],
            outfile=outfile)

@active_if(PARAMS["table_demuxEM"]["active"])
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("celldb.dir/demuxEM.load")
def load_demuxEM(outfile):
    '''load the demuxEM dehashing calls into the database '''

    x = PARAMS["table_demuxEM"]

    DB.load(x["name"],
            x["path"],
            db_url=PARAMS["database_url"],
            glob=x["glob"],
            outfile=outfile)


@follows(load_samples,
         load_gex_qcmetrics,
         load_gex_scrublet,
         load_singleR,
         load_gmm_demux,
         load_demuxEM)
@jobs_limit(PARAMS.get("jobs_limit_db", 1), "db")
@originate("celldb.dir/final.sentinel")
def final(outfile):
    '''
    Construct a "final" view on the database from which
    the cells can be selected and fetched by
    pipeline_fetch_cells.py
    '''

    viewname = "final"
    dbhandle = connect()

    statement = "DROP VIEW IF EXISTS " + viewname
    cc = database.executewait(dbhandle, statement, retries=20)
    cc.close()
    statement = PARAMS["table_final"]["sql_query"]
    cc = database.executewait(dbhandle, statement, retries=20)
    cc.close()
    
    # drop duplicate barcode and library_id columns
    # requires qlite >= 3.35 !
    
    # columns = DB.getColumnNames(dbhandle, viewname)
    
    # cols_to_drop = [x for x in columns 
    #                 if x.startswith("barcode:") or x.startswith("library_id:")]
    
    # for col in cols_to_drop:
    
    #     statement = 'ALTER VIEW %(viewname)s DROP COLUMN %(col)s' % locals()
    #     cc = database.executewait(dbhandle, statement, retries=20)
    #     cc.close()
    

    iotools.touch_file(outfile)


# ########################################################################### #
# ##################### full target: to run all tasks ####################### #
# ########################################################################### #

@follows(final)
def full():
    pass

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
