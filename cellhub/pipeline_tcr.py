'''
======================
pipeline_tcr.py
======================


Overview
========

The pipeline performs a series of tasks aimed at TCR analysis of single cell RNA sequencing data.

It performs the following functions:

* Identifies TCR chain and contigs in each TCR chain for each single cell

* The following will either be performed downstream or implemented in the future:
* (to be done) Performs chain QC, in order to highlight multichain cells and to annotate cell types with specific, distinctive chain configurations
* (to be done) Performes clonotype analysis, that is it calculates diversity indices and lists enriched clonotypes
* (to be done) Performs clustering of CDR3 sequences, to identify clonal neighbours
* (to be done) Identifies whether for each TCR, a known antigenic specificity can be predicted

Usage
=====

See :doc:`Installation</Installation>` and :doc:`Usage</Usage>` for general
information on how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured:file:`pipeline_tcr.yml` file.

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_tcr.py config


Inputs
------

In addition to the "pipeline_tcr.yml" file, the pipeline requires inputs: 

# api/cellranger/ .... tcr (vdj-t) ... 

Dependencies
------------

This pipeline requires:
* cgat-core: https://github.com/cgat-developers/cgat-core
* cellranger: https://support.10xgenomics.com/single-cell-gene-expression/
* apptainer: https://github.com/apptainer/apptainer

Pipeline logic
--------------

The pipeline is designed to perform TCR reconstruction 


Pipeline output
---------------

The pipeline returns:

* a tsv table with barcode metadata and dandelion objects for downstream processing


Code
====

'''

from ruffus import *
from pathlib import Path
import sys
import os
import glob
import re
import sqlite3
import yaml
import csv
import shutil
# from contextlib import contextmanager

import cgatcore.experiment as E
from cgatcore import pipeline as P
import cgatcore.iotools as IOTools

import pandas as pd
import numpy as np

# import local pipeline utility functions
import cellhub.tasks as T
import cellhub.tasks.cellranger as cellranger
import cellhub.tasks.samples as samples

# -------------------------- Pipeline Configuration -------------------------- #


# Override function to collect config files
P.control.write_config_files = T.write_config_files

# load options from the yml file
P.parameters.HAVE_INITIALIZED = False
PARAMS = P.get_parameters(T.get_parameter_file(__file__))

# set the location of the code directory
PARAMS["cellhub_code_dir"] = Path(__file__).parents[1]


if len(sys.argv) > 1:
        if(sys.argv[1] == "config") and __name__ == "__main__":
                    sys.exit(P.main(sys.argv))



# ---------------------------- Pipeline tasks ------------------------------- #

# ########################################################################### #
# ###########################  TCR Analysis  ############################## #
# ########################################################################### #

# 1. Create Scirpy output for TCRs
# Safe to use globbing here as we know these files exist
@transform(glob.glob("api/cellranger/vdj_t/unfiltered/*/all_contig_annotations.csv"), 
regex(r".*/.*/.*/.*/(.*)/all_contig_annotations.csv"),
r"scirpy.dir/\1.scirpy.sentinel")
def scirpyTCR(infile, outfile):
    ''' 
    Create TCR chain table. 
    '''

    outfile_prefix_string = outfile.replace(".sentinel", "") 

    t = T.setup(infile, outfile, PARAMS,
                memory=PARAMS["resources_memory"],
                cpu=PARAMS["resources_cores"])

    statement = '''python %(cellhub_code_dir)s/python/tcr_scirpy.py
                   --contig_path %(infile)s
                   --outfile_prefix %(outfile_prefix_string)s
                    &> %(log_file)s
                 ''' % dict(PARAMS, 
                            **t.var, 
                            **locals())

    P.run(statement, **t.resources)
    IOTools.touch_file(outfile)

#2. Create Dandelion output for TCRs

#  Requires two files per sample as input, and these files aren't available in the api 
#  directory so need to handle them manually. dandelion_setup copies these files to
#  the dandelion.dir directory and also creates a meta.csv file which can be optionally
#  used by dandelion_preprocessing downstream.


@follows(scirpyTCR)
@merge(None, "dandelion.dir/dandelion_setup.sentinel") 
def dandelion_setup(_input, outfile):
    '''
    Setup expected directory structure and files for dandelion input.
    '''
    # Pattern to match directories
    dir_pattern = "cellranger.vdj.t.dir/*/outs/"
    # Compile regex to extract the sample name
    regex = re.compile(r".*/([^/]+)/outs/$")
    
    # Use glob to find directories matching the pattern
    for dir_path in glob.glob(dir_pattern):
        # Use regex to extract the sample name
        match = regex.match(dir_path)
        if match:
            sample_name = match.group(1)
            print(f"Processing {sample_name} in directory {dir_path}")

            # Define the output directory based on the sample name
            output_dir = f"dandelion.dir/{sample_name}/"
            os.makedirs(output_dir, exist_ok=True)

            # # Copy input files to the new directory 
            shutil.copy(os.path.join(dir_path, "all_contig.fasta"), output_dir)
            shutil.copy(os.path.join(dir_path, "all_contig_annotations.csv"), output_dir)
            
            # Symlinked files aren't picked up correctly by dandelion.
            # absolute_dir_path = PARAMS.get("start_dir") + "/" + dir_path
            # # link input files to the new directory 
            # source_file1 = os.path.join(absolute_dir_path, "all_contig.fasta")
            # link_name1 = os.path.join(output_dir, "all_contig.fasta")
            # os.symlink(source_file1, link_name1)
            # source_file2 = os.path.join(absolute_dir_path, "all_contig_annotations.csv")
            # link_name2 = os.path.join(output_dir, "all_contig_annotations.csv")
            # os.symlink(source_file2, link_name2)

            # Append a line to the meta.csv file in the dandelion.dir directory which 
            # can then be used by dandelion to label barcodes per sample (by passing --meta meta.csv).
            # This file can be optionally used by dandelion_preprocessing depending on whether
            # yml dandelion_meta configuration is True or False. In the future we should modify this
            # to include the name of the sample followed by the name of the individual that sample
            # came from.

            string_to_append = f"{sample_name},{sample_name}\n"
            # Define the path to the meta.csv file in the dandelion.dir directory
            meta_csv_path = os.path.join("dandelion.dir", "meta.csv")
            # Open the file in append mode and write the string
            with open(meta_csv_path, "a") as meta_file:
                meta_file.write(string_to_append)

    IOTools.touch_file(outfile)


@follows(dandelion_setup)
@merge(None, "dandelion.dir/dandelion_preprocessing.sentinel") 
def dandelion_preprocessing(_input, outfile):
    '''
    Run dandelion pre-processing using the dandelion apptainer.
    '''
    
    dandelion_dir_path = "dandelion.dir"  # Path to the dandelion.dir directory
    dandelion_dir_path_ab = os.path.abspath(dandelion_dir_path)  # Get the absolute path of the dandelion.dir directory
    meta_csv_path = os.path.join(dandelion_dir_path, "meta.csv")
  
    use_hc = PARAMS.get("dandelion_high_confidence", True)  # Get 'use_hc' from PARAMS, default to True if not found
    use_hc = str(use_hc).lower() != "false"  # Ensures string comparison, accounts for "False", "false", etc.
    hc_argument = ""
    if use_hc:
        print("Using high confidence")
        hc_argument = "--filter_to_high_confidence"


    use_meta = PARAMS.get("dandelion_meta", True)  # Get 'use_meta' from PARAMS, default to True if not found
    use_meta = str(use_meta).lower() != "false"  # Ensures string comparison, accounts for "False", "false", etc.
    meta_argument = ""
    

    # Check if the meta.csv file exists
    if os.path.exists(meta_csv_path):
    # Open the existing file in read mode and read its content
        with open(meta_csv_path, "r") as file:
            content = file.read()
        # Prepend "sample,prefix\n" header to the existing content
        # In the future we should make this sample,individual as we have decided to prefix barcodes downtream.
        new_content = "sample,prefix\n" + content
        # Open the file in write mode and write the new content
        with open(meta_csv_path, "w") as file:
            file.write(new_content)
        if use_meta:
            meta_argument = "--meta meta.csv"
        

    # As we are changing directory we need an updated logfile name
    log_file_simple =  os.path.realpath(outfile.replace(".sentinel", ".log"))
    log_file_simple = f"{os.path.basename(log_file_simple)}"

    t = T.setup(None, outfile, PARAMS,
                memory=PARAMS["dandelion_memory"],
                cpu=PARAMS["dandelion_cores"])


    # Apptainer automatically passes the $HOME environment variable to the container,
    # however it seems that $HOME must have read access permission for "others" to work.
    # Setting the home directory to any other directory which is readable by others seems
    # to fix any errors related to this. Therefore we set home to the dandelion.dir directory.
    # The full commmand therefore looks like this:
    # apptainer run -B $PWD -H /users/sansom/efs143/work/SALL/cellhub/dandelion.dir $DNDLN dandelion-preprocess --chain TR --filter_to_high_confidence --meta meta.csv &> dandelion_preprocessing.log
    # The dandelion module must be loaded for this to work (module load dandelion/0.3.5)
    
    # # Something like this could potentially be used in the future to set memory
    # # and cpu parameters, however it currently doesn't work due to cgroups v2 requirement with the following error:
    # # FATAL:   container creation failed: while applying cgroups config: rootless cgroups requires cgroups v2
    # # Get available memory and convert to bytes to pass to the container
    # size_in_gb = PARAMS.get("dandelion_memory")
    # # Strip the "G" and convert to integer
    # size_in_gb = int(size_in_gb.rstrip('G'))
    # # Convert to bytes
    # size_in_bytes = size_in_gb * (1024 ** 3)
    #
    # Arguments to apptainer:
    #         --memory %(size_in_bytes)s
    #         --cpus %(dandelion_cores)s

    statement = '''
        cd dandelion.dir;
        apptainer run 
        --writable-tmpfs
        -B $PWD
        -H %(dandelion_dir_path_ab)s
        $DNDLN
        dandelion-preprocess
        --chain TR 
        %(hc_argument)s                   
        %(meta_argument)s
        &> %(log_file_simple)s
        ''' % dict(PARAMS,**t.var,**locals())


    # subprocess.run(cmd, shell=True, cwd=dandelion_dir_path, check=True)
    # cwd=dandelion_dir_path_ab doesn't work, need to cd in the statement and update log file location
    P.run(statement, **t.resources) 

    IOTools.touch_file(outfile)



# This doesn't work because globbing is attempted before files exist.
# # For each sample, create a dandelion object and save the object and its metadata to separate files for later use
# #@transform(glob.glob("dandelion.dir/*/dandelion/all_contig_dandelion.tsv"), 
# @follows(dandelion_preprocessing)
# @transform(glob.glob("dandelion.dir/*/dandelion/all_contig_dandelion.tsv"), 
# regex(r"dandelion.dir/(.*)/dandelion/all_contig_dandelion.tsv"),
# r"dandelion.dir/\1.dand3.sentinel")
# 
# Instead performing globbing within function to get the files at runtime
# To Do: make these run in parallel rather than through a for loop

@follows(dandelion_preprocessing)
@transform(dandelion_preprocessing, # Using None as placeholder, actual input determined at runtime
           filter=formatter(), # placeholder
           output=r"dandelion.dir/{subpath[0]}.dandelion.sentinel") # Define output pattern
def dandelionTCR(infile, outfile):
    '''
    Wrapper function to perform runtime globbing and create TCR chain metadata table and dandelion objects
    '''
    # Perform globbing at runtime
    files = glob.glob("dandelion.dir/*/dandelion/all_contig_dandelion.tsv")
    
    for current_file in files:
        # Extract part of the path needed for output filename
        path_parts = current_file.split(os.sep)
        subpath = path_parts[1] # get the sample name
        
        # Define the output file based on the sample name
        outfile = f"dandelion.dir/{subpath}.dandelion.sentinel"

        # Get the prefix for downstream passing to tcr_dandelion
        outfile_prefix_string = outfile.replace(".sentinel", "") 

        t = T.setup(current_file, outfile, PARAMS,
                    memory=PARAMS["resources_memory"],
                    cpu=PARAMS["resources_cores"])

        statement = '''python %(cellhub_code_dir)s/python/tcr_dandelion.py
                       --contig_path %(current_file)s
                       --outfile_prefix %(outfile_prefix_string)s
                        &> %(log_file)s
                     ''' % dict(PARAMS, 
                                **t.var, 
                                **locals())

        P.run(statement, **t.resources)

        IOTools.touch_file(outfile)


# ---------------------------< Pipeline targets >------------------------------- #

@follows(dandelionTCR)
def full():
    '''
    Run the full pipeline.
    '''
    pass


def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))